# Azure AI のインデックスへのアップロード
# 改行が2つ続けばテキストを分割

import os
import re
import fitz  # PyMuPDF
import openai
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.indexes.models import (
    SearchIndex, SimpleField, SearchFieldDataType, SearchableField
)
import base64
import json

# Azure AI SearchとOpenAIの設定
AZURE_SEARCH_ENDPOINT = os.getenv('AZURE_SEARCH_ENDPOINT')
AZURE_SEARCH_API_KEY = os.getenv('AZURE_SEARCH_API_KEY')
AZURE_SEARCH_INDEX_NAME = os.getenv('AZURE_SEARCH_INDEX_NAME')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

openai.api_key = OPENAI_API_KEY

# 環境変数が正しく設定されていることを確認
assert AZURE_SEARCH_ENDPOINT is not None, "AZURE_SEARCH_ENDPOINT is not set"
assert AZURE_SEARCH_API_KEY is not None, "AZURE_SEARCH_API_KEY is not set"
assert AZURE_SEARCH_INDEX_NAME is not None, "AZURE_SEARCH_INDEX_NAME is not set"
assert OPENAI_API_KEY is not None, "OPENAI_API_KEY is not set"

# def extract_text_from_pdf(pdf_path):
#     doc = fitz.open(pdf_path)
#     text = []
#     for page_num in range(doc.page_count):
#         page = doc.load_page(page_num)
#         text.append(page.get_text())
#     return text

def extract_text_from_pdf(pdf_path):
    doc = fitz.open(pdf_path)
    all_text = ""

    # PDF全体のテキストを取得
    for page_num in range(doc.page_count):
        page = doc.load_page(page_num)
        all_text += page.get_text() + "\n"  # ページの終わりで改行を追加

    # 改行が2つ以上続く箇所でテキストを分割
    split_text = re.split(r'\n\s*\n+', all_text)

    return split_text

def create_search_index():
    client = SearchIndexClient(endpoint=AZURE_SEARCH_ENDPOINT, credential=AzureKeyCredential(AZURE_SEARCH_API_KEY))
    fields = [
        SimpleField(name="id", type=SearchFieldDataType.String, key=True),
        SearchableField(name="content", type=SearchFieldDataType.String),
        SimpleField(name="content_vector", type=SearchFieldDataType.String)  # ベクトルを文字列として保存
    ]
    index = SearchIndex(name=AZURE_SEARCH_INDEX_NAME, fields=fields)
    client.create_index(index)

def encode_document_key(key):
    """URLセーフなBase64形式にエンコード"""
    return base64.urlsafe_b64encode(key.encode()).decode()

def generate_embedding(text):
    """OpenAIのAPIを使用してテキストをベクトル化"""
    response = openai.Embedding.create(
        input=text,
        model="text-embedding-ada-002"  # 適切なモデルを選択
    )
    return response['data'][0]['embedding']

def upload_documents(documents):
    client = SearchClient(endpoint=AZURE_SEARCH_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=AzureKeyCredential(AZURE_SEARCH_API_KEY))
    if not documents:
        print("No documents to upload.")
    client.upload_documents(documents=documents)

def main():
    pdf_dir = 'pdfs/'
    documents = []

    for pdf_file in os.listdir(pdf_dir):
        if pdf_file.endswith('.pdf'):
            pdf_path = os.path.join(pdf_dir, pdf_file)
            texts = extract_text_from_pdf(pdf_path)
            for i, text in enumerate(texts):
                document_id = encode_document_key(f"{pdf_file}-{i}")
                content_vector = generate_embedding(text)
                content_vector_str = json.dumps(content_vector)   # テキストをベクトル化
                print(f"Document ID: {document_id}")
                documents.append({
                    "id": document_id,
                    "content": text,
                    "content_vector": content_vector_str  # ベクトルフィールドを追加
                })
    
    print(f"Total documents to upload: {len(documents)}")

    create_search_index()
    upload_documents(documents)

if __name__ == "__main__":
    main()
